#functions for analysis

import os
import numpy as np
import zipfile

import gauops as gps
    
def zip_files(dir_path, zip_file, ext=None):
    '''
    Function for writing all files in a given directory to zip file.
    Optionally matches extensions (used with .sh and .gjf to bundle things for HPC use).
    
    Args:
        dir_path - directory to zip contents of
        zip_file - the zip file we'll write to
        ext      - the extensions allowed for zipping; e.g. .gjf or .sh
    '''
    if not zip_file.endswith('.zip'):
        if len(zip_file.split('.')) == 1:
            zip_file = zip_file + '.zip'
        if len(zip_file.split('.')) != 1:
            zip_file = zip_file.split('.')[0] + '.zip'     
            
    counter = np.array([0,0]) # count up gjf files we zipped {counter[0]} and .sh files {counter[1]}
    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for file in os.listdir(dir_path):
            file_path = os.path.join(dir_path, file)
            if os.path.isfile(file_path) and (ext is None or file.endswith(tuple(ext))):
                zipf.write(file_path, arcname=os.path.basename(file_path))
                counter += np.array([1 * file.endswith('.gjf'), 1 * file.endswith('.sh')]) # update the counter
                
    print(f'A total of {counter[0]} .gjf files and {counter[1]} .sh file(s) were added to into {zip_file}')
    
def process_files(path = os.getcwd(), method=0):
    '''
    Wrapper function - uses the tools above to process output
    
    Args:
        path   - place to get files from
        method - if 0 = use SCF energy, if =1 get complexation energy
    
    Returns:
        data - nested list of data [dx, dy, dz, energy]
    '''
    files = gps.find_log_files(os.getcwd())
    data = []
    print(f"Reading data from {path}")
    for file in files:
        coords = gps.extract_translation_coordinates(file)
        if method == 0:
            energy = gps.extract_final_energy(file)
        if method == 1:
            energy = gps.extract_complexation_energy(file)

        if coords != None and energy != None:
            for coord in coords:
                data.append([coord[0], coord[1], coord[2], energy])
    print(f'Processed {len(data)} configurations from {len(files)} files')
    
    return data
    
def shape_data(data):
    '''
    just shape the data returned by process_files() for plotting
    
    Args:
        data      - the data variable to use
    
    Returns:
        dx (y,z)  - x/y/z translation coordinates
        e_values  - raw energy
        de_values - delta energy
    '''
    
    dx_values = [row[0] for row in data]
    dy_values = [row[1] for row in data]
    dz_values = [row[2] for row in data]
    dyz_values = np.sqrt((np.array([row[1] for row in data])**2) + (np.array([row[2] for row in data])**2))
    e_values = [row[3] for row in data]
    de_values = e_values - np.min(e_values)
    
    return dx_values, dy_values, dz_values, dyz_values, e_values, de_values

def add_npz_extension(filename):
    base, ext = os.path.splitext(filename)
    if ext != '.npz':
        filename = f"{base}.npz"
    return filename

def save_data_to_npz(filename, data):
    filename = add_npz_extension(filename)
    print(f'Saving bimolecular PES data as {filename}')
    np.savez(filename, data = data)
 
def reload_data_from_npz(filename):
    filename = add_npz_extension(filename)
    print(f'Loading bimolecular PES data from {filename}')
    reloaded_data = np.load(filename, allow_pickle = True)
    data = reloaded_data['data']
    reloaded_data.close()
    return data
    
def find_local_minima(data, num_minima=50, e_threshold=5, d_threshold=2):
    """
    Tool for finding local minima by command line.
    
    Args:
        data - data file generated by process_files function
    """
    local_data = np.array(data)
    sorted_indices = np.argsort(local_data[:, 3])
    sorted_data = local_data[sorted_indices]

    minima_list = [sorted_data[0]]

    for point in sorted_data[1:]:
        if point[3] <= (minima_list[0][3] + e_threshold) and len(minima_list) < num_minima:
            add_point = True
            for minima in minima_list:
                distance = np.sqrt(np.sum((point[:3] - minima[:3])**2))
                if distance < d_threshold:
                    add_point = False
                    break
            if add_point:
                minima_list.append(point)
        if len(minima_list) >= num_minima:
            print(f'Found maximum permitted number of local minima ({num_minima})')
            break
            
    return np.array(minima_list)

def find_indices_of_minima(data, minima):
    indices = []

    for minima_row in minima:
        match = np.where((np.array(data)[:, :3] == minima_row[:3]).all(axis=1))[0]
        if match.size > 0:
            indices.append(match[0])
        else:
            indices.append(-1)
    return indices    
    